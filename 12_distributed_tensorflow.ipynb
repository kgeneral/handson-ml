{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext watermark\n",
    "#%watermark -v -p numpy,sklearn,scipy,matplotlib,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***12장 – 분산 텐서플로***\n",
    "\n",
    "ebook : https://books.google.co.kr/books?id=k5daDwAAQBAJ&pg=PA418&lpg=PA418&dq=%ED%95%B8%EC%A6%88%EC%98%A8+%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D+%EC%97%AC%EB%9F%AC+%ED%83%9C%EC%8A%A4%ED%81%AC%EC%97%90+%EC%97%B0%EC%82%B0+%ED%95%A0%EB%8B%B9%ED%95%98%EA%B8%B0&source=bl&ots=r5ow9kWQ4K&sig=4VvemYnt29ikbnEDA9tI6Rfyqk4&hl=ko&sa=X&ved=0ahUKEwjW4pvEj7rcAhUDMt4KHUm6Cn0Q6AEIJjAA#v=onepage&q=%ED%95%B8%EC%A6%88%EC%98%A8%20%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EC%97%AC%EB%9F%AC%20%ED%83%9C%EC%8A%A4%ED%81%AC%EC%97%90%20%EC%97%B0%EC%82%B0%20%ED%95%A0%EB%8B%B9%ED%95%98%EA%B8%B0&f=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목적\n",
    "- 대규모 NN 을 여러 장치에 병렬 / 분산 실행하여 수행 시간을 절약한다.\n",
    "\n",
    "Tensorflow 분산 처리의 장점\n",
    "- 계산 그래프의 여러 장치 / 머신 분할방법 제어 가능\n",
    "- 다양한 방식의 연산 병렬화 및 동기화 가능\n",
    "\n",
    "병렬 수행의 실사례\n",
    "- 신경망 병렬 수행\n",
    "- 모델 세밀 튜닝을 위해 큰 하이퍼파라미터 공간을 탐색\n",
    "- 대규모 NN 의 앙상블 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.1 단일 머신의 다중 장치**\n",
    "\n",
    "- 단일 머신에 GPU 추가\n",
    "- 다중 머신의 경우 네트워크 비용으로 인해 더 비효율적일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.1 설치*\n",
    "\n",
    "- 그래픽 카드 호환성 확인 : https://developer.nvidia.com/cuda-gpus\n",
    "- CUDA : gpu 를 통한 명시적 computing 을 가능하게 하는 library\n",
    "- cuDNN : DNN 을 위한 기초적 GPU 가속 library\n",
    "-- activation function, normalize, forward / back propagation, pooling 등\n",
    "- CUDA, cuDNN 바이너리 다운로드시 nvidia 개발자 계정이 필요 (https://developer.nvidia.com/)\n",
    "\n",
    "* 본 실습에서는 gcp 를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 25 12:23:32 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   47C    P0    72W / 149W |     11MiB / 11441MiB |     50%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   73C    P0    89W / 149W |     11MiB / 11441MiB |     19%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pyenv, anaconda 등 가상 환경을 사용하고 잇다면 적절한 환경으로 활성화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬 2와 3을 모두 지원합니다. 공통 모듈을 임포트하고 맷플롯립 그림이 노트북 안에 포함되도록 설정하고 생성한 그림을 저장하기 위한 함수를 준비합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 2와 파이썬 3 지원\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# 공통\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# 맷플롯립 설정\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 그림을 저장할 폴더\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"distributed\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로컬 서버"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.constant(\"Hello distributed TensorFlow!\")\n",
    "server = tf.train.Server.create_local_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello distributed TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(server.target) as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gpu 초기화 및 정보 로그가 나온다면 성공\n",
    "\n",
    "[I 12:02:07.853 NotebookApp] Adapting to protocol v5.1 for kernel 11e846df-9d53-4d1a-8be6-0622be66099f\n",
    "2018-07-25 12:02:43.034875: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions tha\n",
    "t this TensorFlow binary was not compiled to use: AVX2 FMA\n",
    "2018-07-25 12:02:43.140845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read f\n",
    "rom SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2018-07-25 12:02:43.141396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties\n",
    ": \n",
    "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
    "pciBusID: 0000:00:04.0\n",
    "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
    "2018-07-25 12:02:43.222979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read f\n",
    "rom SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2018-07-25 12:02:43.223631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties\n",
    ": \n",
    "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
    "pciBusID: 0000:00:05.0\n",
    "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
    "2018-07-25 12:02:43.223833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0,\n",
    " 1\n",
    "2018-07-25 12:02:43.774320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecu\n",
    "tor with strength 1 edge matrix:\n",
    "2018-07-25 12:02:43.774378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
    "2018-07-25 12:02:43.774403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n",
    "2018-07-25 12:02:43.774408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n",
    "2018-07-25 12:02:43.774839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/jo\n",
    "b:local/replica:0/task:0/device:GPU:0 with 10761 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id\n",
    ": 0000:00:04.0, compute capability: 3.7)\n",
    "2018-07-25 12:02:43.957038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/jo\n",
    "b:local/replica:0/task:0/device:GPU:1 with 10761 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id\n",
    ": 0000:00:05.0, compute capability: 3.7)\n",
    "2018-07-25 12:02:44.140339: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCa\n",
    "che for job local -> {0 -> localhost:33667}\n",
    "2018-07-25 12:02:44.141491: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:334] Started server with t\n",
    "arget: grpc://localhost:33667\n",
    "2018-07-25 12:02:44.152483: I tensorflow/core/distributed_runtime/master_session.cc:1150] Start master session 6e81\n",
    "0a0c2f63410d with config: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.2 GPU RAM 관리*\n",
    "\n",
    "- 단일 GPU 에 여러 프로그램 수행시 경우에 따라 OOM 발생할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 별도 GPU 에 각각 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA_VISIBLE_DEVICES=0,1 python3 code.py - 복수 device ID 지정시 multi gpu 로 구동되는지 확인\n",
    "#CUDA_VISIBLE_DEVICES=\"\" 의 경우 CPU 로 수행됨\n",
    "\n",
    "# https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter\n",
    "# http://dongjinlee.tistory.com/entry/%EC%84%A0%ED%83%9D%ED%95%9C-GPU%EC%97%90%EB%A7%8C-%EB%A9%94%EB%AA%A8%EB%A6%AC-%ED%95%A0%EB%8B%B9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# do some tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. GPU 메모리 일부만을 사용하도록 강제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 25 12:23:35 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   47C    P0    72W / 149W |  10876MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   74C    P0    88W / 149W |  10876MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      3376      C   /usr/bin/python                            10863MiB |\r\n",
      "|    1      3376      C   /usr/bin/python                            10863MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 필요할 때 메모리를 확장 할당하는 옵션 활용\n",
    "- 비활용 메모리 반납에 시간이 소요되므로(memory fragmentation 을 피하기 위해) 효용성이 낮음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto();\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.3 장치에 연산 배치하기*\n",
    "\n",
    "dynamic placer\n",
    "- 사용자 지정 배치 규칙에 비해 좋은 효율성을 보이고 있지 않다고 함.\n",
    "- G사 내부용으로 현재 오픈소스화 되어 잇지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. simple placer\n",
    "- computation graph 수행시 device 에 배치되지 않는 node 를 평가할 때 사용\n",
    "\n",
    "분배 규칙\n",
    "- node 가 이미 배치되어 있는 장비는 해당 node를 그대로 둠\n",
    "- 사용자가 node 를 어떤 장치에 할당햇다면 placer 가 node 를 배치\n",
    "- GPU #0 이 기본, GPU 가 소진되면 CPU 로 전환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 배치 로깅\n",
    "- placer 가 node 를 배치할 시점에 메세지를 기록하는 옵션\n",
    "- TODO : 출력되는 log 에 대한 부연 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.constant(4.0)\n",
    "\n",
    "c = a * b\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.initializer.run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 동적 배치 함수\n",
    "\n",
    "- tf.device 파라미터로 함수를 넣을 수 있음\n",
    "- 실무에서 자주 사용 되나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_on_cpu(op):\n",
    "    if(op.type == \"Variable\"):\n",
    "        return \"/cpu:0\"\n",
    "    else:\n",
    "        return \"/gpu:0\"\n",
    "\n",
    "with tf.device(variables_on_cpu):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.constant(4.0)\n",
    "    c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 연산과 커널\n",
    "\n",
    "- CPU, GPU 장치별로 지원가능한 연산이 구분되어 잇음\n",
    "- 커널 : 장치에 맞는 구현\n",
    "- 예) integer Variable 은 GPU 에서 지원하지 않음 - 효율성 문제로 강제 미지원\n",
    "- 하시 예시의 경우, dtype=tf.float32 로 명시하지 않으면 숫자 notation 을 통해 정수로 임의판단함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: CPU \nIdentity: GPU CPU \nVariableV2: CPU \n\nColocation members and user-requested devices:\n  Variable_2 (VariableV2) /device:GPU:0\n  Variable_2/Assign (Assign) /device:GPU:0\n  Variable_2/read (Identity) /device:GPU:0\n\nRegistered kernels:\n  device='CPU'\n  device='GPU'; dtype in [DT_INT64]\n  device='GPU'; dtype in [DT_DOUBLE]\n  device='GPU'; dtype in [DT_FLOAT]\n  device='GPU'; dtype in [DT_HALF]\n\n\t [[Node: Variable_2 = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n\nCaused by op 'Variable_2', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-328224bb8331>\", line 2, in <module>\n    a = tf.Variable(3)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 396, in _init_from_args\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 73, in variable_op_v2\n    shared_name=shared_name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 1255, in variable_v2\n    shared_name=shared_name, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: CPU \nIdentity: GPU CPU \nVariableV2: CPU \n\nColocation members and user-requested devices:\n  Variable_2 (VariableV2) /device:GPU:0\n  Variable_2/Assign (Assign) /device:GPU:0\n  Variable_2/read (Identity) /device:GPU:0\n\nRegistered kernels:\n  device='CPU'\n  device='GPU'; dtype in [DT_INT64]\n  device='GPU'; dtype in [DT_DOUBLE]\n  device='GPU'; dtype in [DT_FLOAT]\n  device='GPU'; dtype in [DT_HALF]\n\n\t [[Node: Variable_2 = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: CPU \nIdentity: GPU CPU \nVariableV2: CPU \n\nColocation members and user-requested devices:\n  Variable_2 (VariableV2) /device:GPU:0\n  Variable_2/Assign (Assign) /device:GPU:0\n  Variable_2/read (Identity) /device:GPU:0\n\nRegistered kernels:\n  device='CPU'\n  device='GPU'; dtype in [DT_INT64]\n  device='GPU'; dtype in [DT_DOUBLE]\n  device='GPU'; dtype in [DT_FLOAT]\n  device='GPU'; dtype in [DT_HALF]\n\n\t [[Node: Variable_2 = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-328224bb8331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: CPU \nIdentity: GPU CPU \nVariableV2: CPU \n\nColocation members and user-requested devices:\n  Variable_2 (VariableV2) /device:GPU:0\n  Variable_2/Assign (Assign) /device:GPU:0\n  Variable_2/read (Identity) /device:GPU:0\n\nRegistered kernels:\n  device='CPU'\n  device='GPU'; dtype in [DT_INT64]\n  device='GPU'; dtype in [DT_DOUBLE]\n  device='GPU'; dtype in [DT_FLOAT]\n  device='GPU'; dtype in [DT_HALF]\n\n\t [[Node: Variable_2 = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n\nCaused by op 'Variable_2', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-328224bb8331>\", line 2, in <module>\n    a = tf.Variable(3)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 396, in _init_from_args\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 73, in variable_op_v2\n    shared_name=shared_name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 1255, in variable_v2\n    shared_name=shared_name, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nAssign: CPU \nIdentity: GPU CPU \nVariableV2: CPU \n\nColocation members and user-requested devices:\n  Variable_2 (VariableV2) /device:GPU:0\n  Variable_2/Assign (Assign) /device:GPU:0\n  Variable_2/read (Identity) /device:GPU:0\n\nRegistered kernels:\n  device='CPU'\n  device='GPU'; dtype in [DT_INT64]\n  device='GPU'; dtype in [DT_DOUBLE]\n  device='GPU'; dtype in [DT_FLOAT]\n  device='GPU'; dtype in [DT_HALF]\n\n\t [[Node: Variable_2 = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    a = tf.Variable(3) \n",
    "    \n",
    "session.run(a.initializer) # error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 간접 배치\n",
    "\n",
    "- 해당 커널이 없는 device 에 할당된 연산을, 해당 커널을 가진 임의 device 에 할당하는 옵션\n",
    "- 묵시적 할당이라 프로그래머 예측을 벗어나는 위험이 잇어보이는데...\n",
    "- GPU 가 없을때 CPU 가 지원안하는 커널인 연산을 입력하면? (그런게 잇나?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    a = tf.Variable(3)\n",
    "    \n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "session = tf.Session(config=config)\n",
    "session.run(a.initializer) # /cpu:0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.4 병렬 실행*\n",
    "\n",
    "CPU\n",
    "\n",
    "1) queueing\n",
    "- nn graph 실행시, 먼저 평가할 연산을 찾은 뒤 관련 연산들의 의존도를 측정\n",
    "- 의존성이 전혀 없는 node 들을 할당된 장치의 evaluation queue 에 추가\n",
    "- 하나의 연산이 평가되면 다른 모든 연산의 dependency counter 가 감소\n",
    "- 그 후 dependency counter 가 0 이 되는 연산이 추가로 장치의 evaluation queue 에 추가\n",
    "- 필요한 모든 Node 가 평가되면 output 을 return\n",
    "\n",
    "2) pooling\n",
    "- cpu 의 evaluation queue 에 있는 연산은 inter-op thread pool 로 이동\n",
    "- 각 상황에 맞도록 병렬 처리  \n",
    " - multi-core hardware 를 이용\n",
    " - multithread cpu 커널 \n",
    "  - 여러개의 부분연산으로 분리하여 다른 evaluation queue 에 배치\n",
    "  - 상기 evaluation queue 에 배치된 연산은 (모든 multithread cpu 커널이 공유하는)intra-op thread pool 로 이동\n",
    "\n",
    "- inter / intra-op thread pool 의 thread 수는 옵션으로 조정가능 - default 0 (모든 코어 사용)\n",
    " - 현재 CPU 특정 코어 지정이 불가하기 때문에, 옵션값을 CPU 코어 수 보다 적에 부여해야함\n",
    "\n",
    "GPU\n",
    "- GPU 상의 evaluation queue 연산들은 순서대로 평가됨\n",
    "- 대다수의 연산에 대한 CUDA / cuDNN 기반 multithread GPU 커널 존재\n",
    "\n",
    "\n",
    "* 그림 12-5 삽입 및 관련 설명 추가\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.1.5 제어 의존성*\n",
    "\n",
    "- 의존 연산이 모두 수행되엇음에도, 효율을 위해 evaluation 을 가급적 delay 할 경우\n",
    "-- 다량의 메모리 점유, 다수의 external I/O 발생 등\n",
    "- 다른 연산을 병렬 처리 하며 순차적 실행\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1.0)\n",
    "b = a + 2.0\n",
    "\n",
    "with tf.control_dependencies([a,b]):\n",
    "    x = tf.constant(3.0)\n",
    "    y = tf.constant(4.0)\n",
    "    \n",
    "z = x + y # z 도 a, b 의 evaluation 을 기다리는 의존성이 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.2 다중 머신의 다중 장치**\n",
    "\n",
    "- task : 하나 이상의 텐서플로 서버로 구성\n",
    "- job : 각기 이름이 부여된 task group\n",
    "- cluster : task 라고 불리는 하나 이상의 텐서플로 서버로 구성\n",
    "\n",
    "\n",
    "* 그림 12-6 추가 및 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클러스터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spec = tf.train.ClusterSpec({\n",
    "    \"ps\": [\n",
    "        \"127.0.0.1:2221\",  # /job:ps/task:0\n",
    "        \"127.0.0.1:2222\",  # /job:ps/task:1\n",
    "    ],\n",
    "    \"worker\": [\n",
    "        \"127.0.0.1:2223\",  # /job:worker/task:0 # 외부 장비 ip 로 교체\n",
    "        \"127.0.0.1:2224\",  # /job:worker/task:1\n",
    "        \"127.0.0.1:2225\",  # /job:worker/task:2\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일 머신에서 여러 task 수행은 가능하나 비추천. 각 GPU RAM 점유를 수동으로 조정해줘야함\n",
    "task_ps0 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=0)\n",
    "task_ps1 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=1)\n",
    "task_worker0 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=0)\n",
    "task_worker1 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=1)\n",
    "task_worker2 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=2)\n",
    "\n",
    "# cluster task 설정 시 CUDA_VISIBLE_DEVICES 를 구분지어 설정할 수 있는가?\n",
    "#server.join() ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "cluster_mgpu = tf.train.ClusterSpec({\n",
    "    \"multigpu\": [\n",
    "        \"10.138.0.2:2221\",  # /job:multi-gpu/task:0\n",
    "        \"10.138.0.4:2222\",  # /job:multi-gpu/task:1\n",
    "    ]}\n",
    ")\n",
    "\n",
    "task_mgpu0 = tf.train.Server(cluster_mgpu, job_name=\"multigpu\", task_index=0)\n",
    "task_mgpu1 = tf.train.Server(cluster_mgpu, job_name=\"multigpu\", task_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/job:multigpu/task:0/cpu:0\"):\n",
    "    a = tf.Variable(1.0, name=\"a\")\n",
    "\n",
    "with tf.device(\"/job:multigpu/task:0/gpu:0\"):\n",
    "    b = a + 2\n",
    "\n",
    "with tf.device(\"/job:multigpu/task:1/gpu:1\"):\n",
    "    c = a + b\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "    \n",
    "# 해당 원격 텐서플로 서버에 대한 세션을 오픈, c 를 evaluate 하라는 명령을 전달\n",
    "# 해당 외부 장비의 기본장치(GPU) 에 배치 후 수행, 결과 반환\n",
    "with tf.Session(target=\"grpc://10.138.0.2:2221\", config=config) as sess: \n",
    "#with tf.Session(target=\"grpc://10.138.0.4:2222\", config=config) as sess: \n",
    "    sess.run(a.initializer)\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.1 세션 열기*\n",
    "\n",
    "- 모든 task 가 시작되면 특정 머신의 프로세스의 클라이언트에서 다른 모든 서버에 대해 세션을 열 수 있음\n",
    "- 하기 예시에서, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1.0)\n",
    "b = a + 2\n",
    "c = a * 3\n",
    "\n",
    "# 해당 원격 텐서플로 서버에 대한 세션을 오픈, c 를 evaluate 하라는 명령을 전달\n",
    "# 해당 외부 장비의 기본장치(GPU) 에 배치 후 수행, 결과 반환\n",
    "with tf.Session(\"grpc://127.0.0.1:2223\") as sess: # 외부 장비 ip 로 교체\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.2 마스터와 워커 서비스*\n",
    "\n",
    "- Client / Server 는 gRPC 로 통신함\n",
    " - 적절한 통신 포트를 방화벽에서 열어주어야 함\n",
    "\n",
    "- 텐서플로 서버는 기본적으로 마스터, 워커 서비스를 제공함\n",
    " - 마스터 : 클라이언트가 세션을 열고 그래플르 실행할 수 잇게 해줌\n",
    " - 워커 : 하나의 서버에서 graph 실행을 담당하는 RPC 서비스\n",
    "    \n",
    "- 유연성 제공\n",
    "- 1 Client 가 n Server 에 접속하는 각각 session 오픈 가능\n",
    "- task 마다 1 client 실행\n",
    "- 1 client 로 여러 task 제어\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.3 여러 태스크에 연산 할당하기*\n",
    "\n",
    "- job 이름, task 번호, 장치 유형/번호 지정하여 연산 할당 가능\n",
    "- 장치 유형/번호가 지정되지 않으면 해당 task 의 기본 장치 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여러 디바이스와 서버에 연산을 할당하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(\"/job:ps\"):\n",
    "    a = tf.Variable(1.0, name=\"a\")\n",
    "\n",
    "with tf.device(\"/job:worker\"):\n",
    "    b = a + 2\n",
    "\n",
    "with tf.device(\"/job:worker/task:1\"):\n",
    "    c = a + b\n",
    "    \n",
    "with tf.device(\"/job:ps/task:1/cpu:0\"):\n",
    "    d = a + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(\"grpc://127.0.0.1:2221\") as sess:\n",
    "    sess.run(a.initializer)\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.4 여러 대의 파라미터 서버에 변수를 나누어 분산하기*\n",
    "\n",
    "- 다량의 파라미터가 있는 대규모 모델의 경우, 서버 한 대로 IO가 몰리지 않게 여러 서버에 분산해둠\n",
    "- 별도 명시설정없이 모든 task 에 round-robin 할당해주는 방법 제공\n",
    "- 대체로 파라미터 서버는 파라미터 저장 / 송수신 용도로 사용되고 무거운 연산을 수행하지 않게 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(tf.train.replica_device_setter( # cluster=cluster_spec 처럼 클러스터 명세를 전달해도 ps_tasks 를 찾아서 이용함\n",
    "        ps_tasks=2,\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:worker\")):\n",
    "    v1 = tf.Variable(1.0, name=\"v1\")  # /job:ps/task:0 (defaults to /cpu:0) 에 할당\n",
    "    v2 = tf.Variable(2.0, name=\"v2\")  # /job:ps/task:1 (defaults to /cpu:0) 에 할당\n",
    "    v3 = tf.Variable(3.0, name=\"v3\")  # /job:ps/task:0 (defaults to /cpu:0) 에 할당\n",
    "    s = v1 + v2            # /job:worker (defaults to task:0/cpu:0) 에 할당\n",
    "    with tf.device(\"/task:1\"):\n",
    "        p1 = 2 * s         # /job:worker/task:1 (defaults to /cpu:0) 에 할당\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            p2 = 3 * s     # /job:worker/task:1/cpu:0 에 할당\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "\n",
    "with tf.Session(\"grpc://127.0.0.1:2221\", config=config) as sess:\n",
    "    v1.initializer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.5 리소스 컨테이너를 사용해 여러 세션에서 상태 공유하기*\n",
    "\n",
    "- 여러 \"local session\" 은 서로의 상태를 공유할 수 없음\n",
    "- distributed session 의 경우 variable 의 상태를 클러스터 내부의 resource container 로 관리함\n",
    " - 특정 클라이언트 세션 한 곳에서 새 변수를 생성하면 동일 클러스터 내의 다른 세션에서도 자동으로 사용 가능\n",
    " - 리소스 컨테이너는 master task 에서 관리 되는듯\n",
    "\n",
    "* 그림 12-7 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "\n",
    "x = tf.Variable(0.0, name=\"x\")\n",
    "increment_x = tf.assign(x, x+1)\n",
    "\n",
    "with tf.Session(\"grpc://127.0.0.1:2221\", config=config) as sess:\n",
    "    sess.run(x.initializer)\n",
    "    print(x.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(\"grpc://127.0.0.1:2222\", config=config) as sess:\n",
    "    sess.run(increment_x)\n",
    "    print(x.eval())\n",
    "\n",
    "# 다른 장비에서 볼 수 있도록 cluster 를 2대 machine 으로 구성해볼것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.variable_scope(\"test_01\"):\n",
    "\n",
    "#with tf.container(\"test_01\")\n",
    "\n",
    "#tf.Session.reset(\"grpc://127.0.0.1:2221\", [\"test_01\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.6 텐서플로 큐를 사용한 비동기 통신*\n",
    "\n",
    "- 비동기 데이터 교환을 가능하게 함\n",
    "-- 데이터 로더 -> 큐 -> 데이터 학습\n",
    "\n",
    "- placeholder 를 사용해 클러스터에 데이터 주입\n",
    "\n",
    "* 그림 12.8 추가\n",
    "\n",
    "- FIFOQueue\n",
    "-- tuple 지원\n",
    "- RandomShuffleQueue\n",
    "- PaddingFIFOQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "\n",
    "# tuple\n",
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[[2]], name=\"q\", shared_name=\"shared_q\")\n",
    "\n",
    "training_instance = tf.placeholder(tf.float32, shape=(2))\n",
    "enqueue = q.enqueue([training_instance]) \n",
    "\n",
    "training_instances = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "enqueue_many = q.enqueue_many([training_instance]) \n",
    "\n",
    "with tf.Session(\"grpc://127.0.0.1:2221\", config=config) as sess:\n",
    "    #sess.run(enqueue, feed_dict={training_instance: [1., 2.]})\n",
    "    sess.run(enqueue_many, feed_dict={training_instance: [1., 2.], [3., 4.]})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "\n",
    "# tuple\n",
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[[2]], name=\"q\", shared_name=\"shared_q\")\n",
    "\n",
    "dequeue = q.dequeue() \n",
    "\n",
    "batch_size=2\n",
    "dequeue_many = q.dequeue_many(batch_size) \n",
    "\n",
    "with tf.Session(\"grpc://127.0.0.1:2222\", config=config) as sess:\n",
    "    #sess.run(dequeue)\n",
    "    sess.run(dequeue_many)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "6.0\n",
      "3.0\n",
      "4.0\n",
      "dequeue 타임 아웃\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[()])\n",
    "v = tf.placeholder(tf.float32)\n",
    "enqueue = q.enqueue([v])\n",
    "dequeue = q.dequeue()\n",
    "output = dequeue + 1\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.operation_timeout_in_ms = 1000\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(enqueue, feed_dict={v: 1.0})\n",
    "    sess.run(enqueue, feed_dict={v: 2.0})\n",
    "    sess.run(enqueue, feed_dict={v: 3.0})\n",
    "    print(sess.run(output))\n",
    "    print(sess.run(output, feed_dict={dequeue: 5}))\n",
    "    print(sess.run(output))\n",
    "    print(sess.run(output))\n",
    "    try:\n",
    "        print(sess.run(output))\n",
    "    except tf.errors.DeadlineExceededError as ex:\n",
    "        print(\"dequeue 타임 아웃\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.2.7 그래프에서 직접 데이터 로드하기*\n",
    "\n",
    "- 여러번 전송이 일어나서 대규모 환경에서는 비효율적\n",
    " - 파일IO > 클라이언트 > 마스터 태스크 > 데이터를 필요로하는 다른 태스크\n",
    "- 여러 클라이언트가 동시레 한 자원(데이터) 을 사용할때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터를 변수에 프리로드하기\n",
    "- 훈련 데이터를 한번에 로드하여 변수에 할당(메모리 크기에 맞으면)\n",
    " - 클라이언트 > 클러스터 1회 전송\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 그래프에서 직접 훈련 데이터 읽기\n",
    "- 훈련 데이터가 메모리 크기에 안 맞다면 reader 활용\n",
    "- 파일 시스템에서 직접 데이터를 읽음(클라이언트를 통하지 않고 가능)\n",
    "- 지원 형식 : CSV, fixed length binary record, TFRecords\n",
    "\n",
    "* 12-9 그림 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "더 이상 읽을 파일이 없습니다\n",
      "[array([[ 4.,  5.],\n",
      "       [ 1., -1.]], dtype=float32), array([1, 0], dtype=int32)]\n",
      "[array([[7., 8.]], dtype=float32), array([0], dtype=int32)]\n",
      "더 이상 훈련 샘플이 없습니다\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "test_csv = open(\"my_test.csv\", \"w\")\n",
    "test_csv.write(\"x1, x2 , target\\n\")\n",
    "test_csv.write(\"1.,, 0\\n\")\n",
    "test_csv.write(\"4., 5. , 1\\n\")\n",
    "test_csv.write(\"7., 8. , 0\\n\")\n",
    "test_csv.close()\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "features = tf.stack([x1, x2])\n",
    "\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "    capacity=10, min_after_dequeue=2,\n",
    "    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "    name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "enqueue_instance = instance_queue.enqueue([features, target])\n",
    "close_instance_queue = instance_queue.close()\n",
    "\n",
    "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    sess.run(close_filename_queue)\n",
    "    try:\n",
    "        while True:\n",
    "            sess.run(enqueue_instance)\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"더 이상 읽을 파일이 없습니다\")\n",
    "    sess.run(close_instance_queue)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([minibatch_instances, minibatch_targets]))\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"더 이상 훈련 샘플이 없습니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리더 (Reader) - 예전 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 6, 44]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "default1 = tf.constant([5.])\n",
    "default2 = tf.constant([6])\n",
    "default3 = tf.constant([7])\n",
    "dec = tf.decode_csv(tf.constant(\"1.,,44\"),\n",
    "                    record_defaults=[default1, default2, default3])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coord = tf.train.Coordinator()\n",
    "#threads = tf.train.start_queue_runners(coord=coord)\n",
    "#filename_queue = tf.train.string_input_producer([\"test.csv\"])\n",
    "#coord.request_stop()\n",
    "#coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Coordinator 와 QueueRunner 를 사용하는 멀티스레드 리더\n",
    "\n",
    "- 여러 스레드가 여러 리더로 여러 파일을 동시에 read 할 목적\n",
    "- 스레드 구현 및 제어 관리를 신경쓰지 않고 사용 가능\n",
    "\n",
    "* 그림 12-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QueueRunner와 Coordinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 7.,  8.],\n",
      "       [ 1., -1.]], dtype=float32), array([0, 0], dtype=int32)]\n",
      "[array([[4., 5.]], dtype=float32), array([1], dtype=int32)]\n",
      "더 이상 훈련 샘플이 없습니다\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "features = tf.stack([x1, x2])\n",
    "\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "    capacity=10, min_after_dequeue=2,\n",
    "    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "    name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "enqueue_instance = instance_queue.enqueue([features, target])\n",
    "close_instance_queue = instance_queue.close()\n",
    "\n",
    "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
    "\n",
    "n_threads = 5\n",
    "queue_runner = tf.train.QueueRunner(instance_queue, [enqueue_instance] * n_threads)\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    sess.run(close_filename_queue)\n",
    "    enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([minibatch_instances, minibatch_targets]))\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"더 이상 훈련 샘플이 없습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 4.,  5.],\n",
      "       [ 1., -1.]], dtype=float32), array([1, 0], dtype=int32)]\n",
      "[array([[7., 8.]], dtype=float32), array([0], dtype=int32)]\n",
      "더 이상 훈련 샘플이 없습니다\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "def read_and_push_instance(filename_queue, instance_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "    features = tf.stack([x1, x2])\n",
    "    enqueue_instance = instance_queue.enqueue([features, target])\n",
    "    return enqueue_instance\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "    capacity=10, min_after_dequeue=2,\n",
    "    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "    name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "\n",
    "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
    "\n",
    "read_and_enqueue_ops = [read_and_push_instance(filename_queue, instance_queue) for i in range(5)]\n",
    "queue_runner = tf.train.QueueRunner(instance_queue, read_and_enqueue_ops)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    sess.run(close_filename_queue)\n",
    "    coord = tf.train.Coordinator()\n",
    "    enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([minibatch_instances, minibatch_targets]))\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"더 이상 훈련 샘플이 없습니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 타임아웃 지정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 1.4에서 소개된 Data API를 사용하면 손쉽게 데이터를 효율적으로 읽을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0에서 9까지 정수를 세 번 반복한 간단한 데이터셋을 일곱 개씩 배치로 만들어 시작해 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "dataset = dataset.repeat(3).batch(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 줄은 0에서 9까지 정수를 담은 데이터셋을 만듭니다. 두 번째 줄은 이 데이터셋의 원소를 세 번 반복하고 일곱 개씩 담은 새로운 데이터셋을 만듭니다. 위에서 볼 수 있듯이 원본 데이터셋에서 여러 변환 메서드를 연결하여 호출하여 적용했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그다음, 데이터셋을 한 번 순회하는 원-샷-이터레이터(one-shot-iterator)를 만들고, 다음 원소를 지칭하는 텐서를 얻기 위해 `get_next()` 메서드를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`next_element`를 반복적으로 평가해서 데이터셋을 순회해 보죠. 원소가 별로 없기 때문에 `OutOfRangeError`가 발생합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "[7 8 9 0 1 2 3]\n",
      "[4 5 6 7 8 9 0]\n",
      "[1 2 3 4 5 6 7]\n",
      "[8 9]\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(next_element.eval())\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋네요! 잘 작동합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "늘 그렇듯이 텐서는 그래프를 실행(`sess.run()`)할 때마다 한 번만 평가된다는 것을 기억하세요. `next_element`에 의존하는 텐서를 여러개 평가하더라도 한 번만 평가됩니다. 또한 `next_element`를 동시에 두 번 실행해도 마찬가지입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2, 3, 4, 5, 6]), array([0, 1, 2, 3, 4, 5, 6])]\n",
      "[array([7, 8, 9, 0, 1, 2, 3]), array([7, 8, 9, 0, 1, 2, 3])]\n",
      "[array([4, 5, 6, 7, 8, 9, 0]), array([4, 5, 6, 7, 8, 9, 0])]\n",
      "[array([1, 2, 3, 4, 5, 6, 7]), array([1, 2, 3, 4, 5, 6, 7])]\n",
      "[array([8, 9]), array([8, 9])]\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([next_element, next_element]))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`interleave()` 메서드는 강력하지만 처음에는 이해하기 좀 어렵습니다. 예제를 통해 이해하는 것이 가장 좋습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "dataset = dataset.interleave(\n",
    "    lambda v: tf.data.Dataset.from_tensor_slices(v),\n",
    "    cycle_length=3,\n",
    "    block_length=2)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,7,8,4,5,2,3,9,0,6,7,4,5,1,2,8,9,6,3,0,1,2,8,9,3,4,5,6,7,완료\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(next_element.eval(), end=\",\")\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cycle_length=3`이므로 새로운 데이터셋은 이전 데이터셋에서 세 개의 원소를 추출합니다. 즉 `[0,1,2,3,4,5,6]`, `[7,8,9,0,1,2,3]`, `[4,5,6,7,8,9,0]` 입니다. 그다음 원소마다 하나의 데이터셋을 만들기 위해 람다(lambda) 함수를 호출합니다. `Dataset.from_tensor_slices()`를 사용했기 때문에 각 데이터셋은 차례대로 원소를 반환합니다. 다음 이 세 개의 데이터셋에서 각각 두 개의 아이템(`block_length=2`이므로)을 추출합니다. 세 개의 데이터셋의 아이템이 모두 소진될 때까지 반복됩니다. 즉 0,1 (첫 번째에서), 7,8 (두 번째에서), 4,5 (세 번째에서), 2,3 (첫 번째에서), 9,0 (두 번째에서) 등과 같은 식으로 8,9 (세 번째에서), 6 (첫 번째에서), 3 (두 번째에서), 0 (세 번째에서)까지 진행됩니다. 그다음에 원본 데이터셋에서 다음 번 세 개의 원소를 추출하려고 합니다. 하지만 두 개만 남아 있습니다. `[1,2,3,4,5,6,7]`와 `[8,9]` 입니다. 다시 이 원소로부터 데이터셋을 만들고 이 데이텃세의 아이템이 모두 소진될 때까지 두 개의 아이템을 추출합니다. 1,2 (첫 번째에서), 8,9 (두 번째에서), 3,4 (첫 번째에서), 5,6 (첫 번째에서), 7 (첫 번째에서)가 됩니다. 배열의 길이가 다르기 때문에 마지막에는 교대로 배치되지 않았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리더 (Reader) - 새로운 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_tensor_slices()`나 `from_tensor()`를 기반으로 한 원본 데이터셋을 사용하는 대신 리더 데이터셋을 사용할 수 있습니다. 복잡한 일들을 대부분 대신 처리해 줍니다(예를 들면, 스레드):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"my_test.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 줄을 어떻게 디코드해야 하는지는 알려 주어야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_csv_line(line):\n",
    "    x1, x2, y = tf.decode_csv(\n",
    "        line, record_defaults=[[-1.], [-1.], [-1.]])\n",
    "    X = tf.stack([x1, x2])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그다음, 이 디코딩 함수를 `map()`을 사용하여 데이터셋에 있는 각 원소에 적용할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.skip(1).map(decode_csv_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 원-샷-이터레이터를 만들어 보죠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = dataset.make_one_shot_iterator()\n",
    "X, y = it.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1.] 0.0\n",
      "[4. 5.] 1.0\n",
      "[7. 8.] 0.0\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            X_val, y_val = sess.run([X, y])\n",
    "            print(X_val, y_val)\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.3 텐서플로 클러스터에서 신경망 병렬화하기**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.3.1 장치마나 하나의 신경망*\n",
    "\n",
    "- 가장 단순함, 직관적\n",
    "- 여러 하이퍼파라미터를 돌려보며 튜닝하는 목적\n",
    "- realtime prediction 전용 환경으로도 좋음(gpu 를 증가시켜 scale-out 가능)\n",
    "\n",
    "- tensorflow serving\n",
    "\n",
    "* 그림 12-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.3.2 그래프 내 복제와 그래프 간 복제*\n",
    "\n",
    "- 여러 신경망을 분산배치하여 앙상블을 구성할 수 잇음\n",
    "- 모든 신경망들의 prediction 을 모아서 앙상블의 prediction 이 가능\n",
    "- 앙상블 구성방법\n",
    " - 그래프 내 복제 : 구현이 간단함\n",
    " - 그래프 간 복제 : 구현이 복잡하지만, queue 통신 기반이기 때문에 앙상블의 안정성을 좀 더 보장해줌\n",
    "\n",
    "\n",
    "* 그림 12-12, 12-13\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.3.3 모델 병렬화*\n",
    "\n",
    "- 모델을 여러 부분으로 나누어 각 부분을 다른 장치에서 실행\n",
    "- 신경망 모델 구조에 따라 효율성과 구현 난이도가 갈림\n",
    "\n",
    "* 그림 12-14, 15, 16\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*12.3.4 데이터 병렬화*\n",
    "\n",
    "- 각 장치에 모델 복제 후 각각 다른 데이터(미니배치) 를 사용\n",
    "- 훈련 후 발생하는 gradient 를 취합 후 모델 파라미터 업데이트\n",
    " - 동기 / 비동기\n",
    "\n",
    "* 12-17\n",
    "\n",
    "1. 동기\n",
    "\n",
    "- 모든 gradient 가 계산될때까지 대기한 후 평균을 계산\n",
    "- 거의 같은 시점에 모든 복제모델에 파라미터가 업데이트 \n",
    "\n",
    "2. 비동기\n",
    "\n",
    "- gradient 가 계산될 시점에 그때그때 모델 파라미터 업데이트\n",
    "- 훈련학습이 빨리되는 모델이 더 파라미터를 자주 업데이트하게 됨\n",
    "- stale gradient\n",
    " - gradient 가 계산된 후 그 값으로 파라미터가 업데이트 되기 전에 다른 모델이 파라미터를 업데이트 한다면?\n",
    "\n",
    "* 12-18\n",
    "\n",
    "3. 대역폭 포화\n",
    "\n",
    "- gpu ram i/o 시간 > gpu 연산 계산시간(분할된)\n",
    " - 네트워크나 버스로?\n",
    "- 모델이 규모가 작고 데이터 량이 많다면 1 gpu 1 machine 이 더 낫다고함\n",
    "\n",
    "- 대역폭 감소 방안\n",
    " - 적은 수의 머신에 GPU 를 모아서 네트워크 통신 최소화\n",
    " - 여러 대의 파라미터 서버에 파라미터 분산\n",
    " - 모델 파라미터 정밀도 조정 (float32 -> float16)\n",
    " \n",
    "4. 텐서플로 구현 \n",
    "\n",
    "- 그래프 내 복제 / 그래프 간 복제, 동기 / 비동기 등 방법 선택\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 연습문제 해답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coming soon**\n",
    "\n",
    "https://github.com/ageron/handson-ml/issues/187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 텐서플로 프로그램 수행시 CUDA_ERROR_OUT_OF_MEMORY 오류가 발생하는 원인 및 해결책은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (장치) 연산 할당 / 연산 배치 의 차이점."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GPU 버젼 텐서플로의 기본 배치로 실행하면 모든 연산이 GPU #0 에 배치되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. \"/gpu:0\" 에 할당된 Variable 을 \"/gpu:1\" 상의 연산이 사용 할 수 있는가?\n",
    "\n",
    "    또는 \"/cpu:0\" 상의 연산이 사용 할 수 있는가? 다른 서버 장치에 할당된 연산은?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 같은 장치에 배치된 두 연산이 동시에 수행될 수 있는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 제어 의존성이 무엇인가? 언제 사용되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 클러스터로 며칠간 DNN 을 훈련한 후 Saver 로 모델을 저장하지 않앗다면 모델은 날아간 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 클러스터에서 몇 개의 DNN 을 각기 다른 hparam 으로 훈련해 보라.\n",
    "   cross validation 또는 validation set 을 통해 가장 좋은 모델 세 개를 선택하라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 상기 선택된 모델 세 개로 앙상블을 구성하라. 개별 DNN 보다 성능이 더 나은가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 그래프 간 복제, 비동기 업데이트를 통해 DNN 을 훈련 시켜보라. 동기 업데이트를 사용해 다시 훈련해보라.\n",
    "    성능 및 훈련 소요 시간을 비교해보라.\n",
    "    DNN 수직 분할 후 훈련시켰을 때도 추가로 비교해 보라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
